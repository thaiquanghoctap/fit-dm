{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lab03: Frequent itemset mining\n",
    "\n",
    "- Student ID: 21120123\n",
    "- Student name: Lê Thanh Thái Quảng\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) # Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) # a python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**TODO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a, b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space)\n",
    "\n",
    "    -------------------\n",
    "    return\n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # TODO (hint: this function will be called in generateSearchSpace method.):\n",
    "    ret = set(a) | set(b)\n",
    "    return ret\n",
    "\n",
    "class TP:\n",
    "    def __init__(self, data=None, s=None, minSup=None):\n",
    "        self.data = data\n",
    "        self.s = {}\n",
    "\n",
    "        for key, support in sorted(s.items(), key=lambda item: item[1]):\n",
    "            self.s[key] = support\n",
    "        # TODO: why should we do this, answer it at the markdown below?\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.L = {}  # Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree = {}\n",
    "\n",
    "        search_space = {}\n",
    "        for item, support in self.s.items():\n",
    "            search_space[item] = {}\n",
    "\n",
    "            search_space[item]['itemset'] = [item]\n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO:   \n",
    "            # answer at the markdown below.\n",
    "\n",
    "            search_space[item]['pruned'] = False\n",
    "            # TODO:\n",
    "            # After finish implementing the algorithm tell me why should you use this\n",
    "            # instead of delete item directly from search_space and tree.\n",
    "\n",
    "            search_space[item]['support'] = support\n",
    "\n",
    "            tree[item] = {}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "\n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            '''\n",
    "\n",
    "        return tree, search_space\n",
    "\n",
    "    def computeItemsetSupport(self, itemset):\n",
    "\n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        # Tính support khi đưa itemset vào và đếm trong data, data là 1 dict với key là tid và value là items (items là set)\n",
    "        support = 0\n",
    "        for items in self.data.values():\n",
    "            if set(itemset) <= items:\n",
    "                support += 1\n",
    "        return support\n",
    "\n",
    "    def get_sub_tree(self, k, tree, search_space, itter_node):\n",
    "        if k == 0:\n",
    "            return search_space[itter_node]['support']\n",
    "        subtree = search_space[itter_node]\n",
    "        for node in subtree.keys():\n",
    "            k-=1\n",
    "            self.get_sub_tree(k,tree,search_space,node)\n",
    "\n",
    "\n",
    "    def prune(self, k, tree, search_space):\n",
    "\n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent\n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets.\n",
    "        '''\n",
    "        if self.L.get(k) is None: self.L[k] = []\n",
    "        # TODO\n",
    "        # search_space là dict với key là item, \n",
    "        for itemset in search_space.keys():\n",
    "            if search_space[itemset][\"support\"] > self.minSup:\n",
    "                self.L[k].append(search_space[itemset][\"itemset\"])\n",
    "            else:\n",
    "                search_space[itemset][\"pruned\"] = True\n",
    "\n",
    "    def generateSearchSpace(self, k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function)\n",
    "        '''\n",
    "        items = list(tree.keys())\n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l = len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l == 0: return  # Stop condition\n",
    "        for i in range(l - 1):\n",
    "            sub_search_space = {}\n",
    "            sub_tree = {}\n",
    "            a = items[i]\n",
    "            if search_space[a]['pruned']: continue\n",
    "\n",
    "            for j in range(i + 1, l):\n",
    "                b = items[j]\n",
    "                search_space[a][b] = {}\n",
    "                tree[a][b] = {}\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft).\n",
    "\n",
    "                # TODO:\n",
    "                # First create newset using join set\n",
    "                if k == 1:\n",
    "                    a_ = [a]\n",
    "                    b_ = [b]\n",
    "                else:\n",
    "                    a_ = a\n",
    "                    b_ = b\n",
    "                newset = joinset(a_, b_)\n",
    "                \n",
    "                # Second add newset to search_space\n",
    "                sub_tree[tuple(newset)] = {}\n",
    "                sub_search_space[tuple(newset)] = {\n",
    "                    \"itemset\": newset,\n",
    "                    \"pruned\": False,\n",
    "                    \"support\": self.computeItemsetSupport(newset)\n",
    "                }\n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k + 1, sub_tree, sub_search_space)\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree, search_space = self.initialize()  # generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "\n",
    "    def miningResults(self):\n",
    "        return self.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "data, s= readData('data/chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnxbU77YRjaF",
    "outputId": "c3b158be-6b46-4a3c-9b71-6a92d3d31ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [[48], [56], [66], [34], [62], [7], [36], [60], [40], [29], [52], [58]], 2: [{48, 52}, {48, 58}, {56, 29}, {56, 52}, {56, 58}, {66, 60}, {66, 29}, {66, 52}, {66, 58}, {40, 34}, {34, 29}, {34, 52}, {34, 58}, {60, 62}, {40, 62}, {29, 62}, {52, 62}, {58, 62}, {60, 7}, {40, 7}, {29, 7}, {52, 7}, {58, 7}, {36, 60}, {40, 36}, {36, 29}, {36, 52}, {58, 36}, {40, 60}, {60, 29}, {60, 52}, {58, 60}, {40, 29}, {40, 52}, {40, 58}, {52, 29}, {58, 29}, {58, 52}], 3: [{48, 58, 52}, {56, 52, 29}, {56, 58, 29}, {56, 58, 52}, {66, 60, 29}, {66, 60, 52}, {66, 60, 58}, {66, 52, 29}, {66, 58, 29}, {66, 52, 58}, {40, 34, 29}, {40, 34, 52}, {40, 34, 58}, {34, 52, 29}, {34, 58, 29}, {34, 52, 58}, {60, 29, 62}, {60, 62, 52}, {58, 60, 62}, {40, 29, 62}, {40, 52, 62}, {40, 58, 62}, {52, 29, 62}, {58, 29, 62}, {58, 52, 62}, {40, 60, 7}, {60, 29, 7}, {60, 52, 7}, {58, 60, 7}, {40, 29, 7}, {40, 52, 7}, {40, 58, 7}, {52, 29, 7}, {58, 29, 7}, {58, 52, 7}, {40, 36, 60}, {36, 29, 60}, {36, 60, 52}, {58, 36, 60}, {40, 36, 29}, {40, 36, 52}, {40, 58, 36}, {36, 29, 52}, {58, 36, 29}, {58, 36, 52}, {40, 60, 29}, {40, 60, 52}, {40, 58, 60}, {60, 29, 52}, {58, 60, 29}, {58, 60, 52}, {40, 52, 29}, {40, 58, 29}, {40, 58, 52}, {58, 52, 29}], 4: [{66, 52, 60, 29}, {66, 58, 60, 29}, {66, 52, 58, 60}, {66, 52, 58, 29}, {34, 52, 40, 29}, {34, 40, 58, 29}, {34, 52, 40, 58}, {34, 52, 58, 29}, {58, 60, 29, 62}, {52, 58, 60, 62}, {52, 40, 29, 62}, {40, 58, 29, 62}, {52, 40, 58, 62}, {52, 58, 29, 62}, {7, 40, 58, 60}, {52, 7, 60, 29}, {7, 58, 60, 29}, {52, 7, 58, 60}, {52, 7, 40, 29}, {7, 40, 58, 29}, {52, 7, 40, 58}, {52, 7, 58, 29}, {36, 40, 60, 29}, {36, 52, 40, 60}, {36, 40, 58, 60}, {36, 52, 60, 29}, {36, 58, 60, 29}, {36, 52, 58, 60}, {36, 52, 40, 29}, {36, 40, 58, 29}, {36, 52, 40, 58}, {36, 52, 58, 29}, {52, 40, 60, 29}, {40, 58, 60, 29}, {52, 40, 58, 60}, {52, 58, 60, 29}, {52, 40, 58, 29}], 5: [{66, 52, 58, 60, 29}, {34, 40, 52, 58, 29}, {40, 52, 58, 29, 62}, {7, 52, 58, 60, 29}, {7, 40, 52, 58, 29}, {36, 40, 52, 60, 29}, {36, 40, 58, 60, 29}, {36, 40, 52, 58, 60}, {36, 52, 58, 60, 29}, {36, 40, 52, 58, 29}, {40, 52, 58, 60, 29}], 6: [{36, 40, 52, 58, 60, 29}]}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "a=TP(data=data,s=s, minSup=3000)\n",
    "print(a.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n",
    "**Why don't we compute support of items while reading data?**\n",
    "- Trong quá trình đọc dữ liệu, đối với lớn có thể tạo rất rất nhiều itemset với 1, 2, 3, ... phần tử, nếu như tính support của toàn bộ thì rất tốn kém bởi không phải toàn bộ itemset đều là phổ biến. Thay vào đó trong quá trình đọc, sau khi đọc 1 dòng transaction, chỉ cần tính support của item trên dòng đó.\n",
    "\n",
    "**why should we do sort**\n",
    "- Sắp xếp các itemset trong ```self.s``` theo thứ tự tăng dần của support. Việc sắp xếp các itemset theo thứ tự tăng dần của support giúp trong việc tối ưu hóa ```prune``` vì có thể giảm thiểu số lượng phép so sánh cần thực hiện và tăng hiệu suất của thuật toán.\n",
    "\n",
    "**study about python set and its advantages ?**\n",
    "- Python set là tập hợp các phần tử không trùng nhau và không có thứ tự (không thể truy cập phần tử bằng chỉ số). Set cung cấp các phép toán tập hợp như: giao, hợp, hiệu, ...\n",
    "- Lợi ích của set: Loại bỏ các phần tử trùng lặp tự động, giúp ích khi join 2 set. Không chứa phần tử trùng nhau nên khi đếm support không phải xử lí trường hợp trùng nhau. Xác định dễ dàng 1 set có xuất hiện trong 1 set khác hay không.\n",
    "\n",
    "**After finish implementing the algorithm tell me why should you use this? Instead of delete item directly from search_space and tree.**\n",
    "- Chỉ cần đánh dấu pruned=\"True\" hay pruned=\"False\" các itemset không phổ biến mà không cần phải xóa khỏi search_space và tree; khi duyệt cây chỉ cần bỏ qua nhánh có đánh dấu pruned=\"True\". Việc xóa các itemset này có thể có thể có sai sót trong qua trình thực hiện thuật toán, thuật toán này là thuật toán đệ qui nên trong quá trình có thể sẽ có sử dụng đến các nhanh bị đánh dấu pruned=\"True\" (Cụ thể tại câu lệnh \" if search_space[a]['pruned'])\n",
    "\n",
    "**Apriori algorithm and Tree Projection, tell me the differences of two algorithms.**\n",
    "- Apriori tìm các tập phổ biến bằng cách: tạo tập ứng viên có $k$ phần tử từ các tập phổ biến có $k-1$ sau đó tính support của các tập ứng viên rồi chọn ra các tập phổ biến. Như vậy với Apriori số lượng tập ứng viên tạo ra rất nhiều.\n",
    "- Tree Projection sử dụng cấu trúc cây để tìm tập phổ biến. Đầu tiên xây dựng một cây mà mỗi nút của cây tương ứng với một item trong tập dữ liệu ban đầu. Tại một nút ở cây có itemset có độ dài $k$, nếu support của itemset đó không thỏa minsup thì nút đó sẽ bị pruned=\"True\", nếu thỏa minsup thì join itemset tại nút đó với itemset của lần lượt nút có cùng độ sâu để tạo thành itemset mới có độ dài $k+1$. Tiếp tục đến nút mới tạo ra và lặp lại tương tự (đệ qui). Như vậy thuật toán sẽ không tạo ra toàn bộ các itemset ứng viên và thuật toán chỉ duyệt qua dữ liệu một lần duy nhất.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). \n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData2(path):\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0)\n",
    "    with open(path,'rt') as f:\n",
    "        attrs = f.readline().split(\",\")\n",
    "        attrs[-1] = attrs[-1].rstrip(\"?\\n\")\n",
    "        tid=1;\n",
    "        for line in f:\n",
    "            values = line.split(\",\")\n",
    "            values[-1] = values[-1].rstrip(\".\\n\")\n",
    "\n",
    "            # itemset là 1 string có được bằng cách ghép tên thuộc tính và giá trị\n",
    "            # lý do: Có thuộc tính có thể có cùng miền giá trị\n",
    "            # ví dụ: Int'l Plan, VMail Plan đều có miền giá trị là {yes, no}\n",
    "            # Nếu như cả Int'l Plan, VMail Plan đều có giá trị \"no\" khi tạo set sẽ bị mất 1 giá trị \"no\",\n",
    "            # mặc dù ý nghĩa Int'l Plan, VMail Plan là khác nhau\n",
    "            itemset = [f\"{attrs[i]}={values[i]}\" for i in range(len(attrs))]\n",
    "            itemset = set(itemset)\n",
    "            \n",
    "            for item in itemset:  \n",
    "                s[item]+=1     \n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    \n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [['CustServ Calls=1'], ['Area Code=415'], ['VMail Message=0'], ['VMail Plan=no'], ['Churn=False'], [\"Int'l Plan=no\"]], 2: [{'Churn=False', 'CustServ Calls=1'}, {'CustServ Calls=1', \"Int'l Plan=no\"}, {'Area Code=415', 'VMail Message=0'}, {'Area Code=415', 'VMail Plan=no'}, {'Churn=False', 'Area Code=415'}, {'Area Code=415', \"Int'l Plan=no\"}, {'VMail Message=0', 'VMail Plan=no'}, {'Churn=False', 'VMail Message=0'}, {'VMail Message=0', \"Int'l Plan=no\"}, {'Churn=False', 'VMail Plan=no'}, {'VMail Plan=no', \"Int'l Plan=no\"}, {'Churn=False', \"Int'l Plan=no\"}], 3: [{'Area Code=415', 'VMail Message=0', 'VMail Plan=no'}, {'Area Code=415', 'VMail Message=0', \"Int'l Plan=no\"}, {'Area Code=415', 'VMail Plan=no', \"Int'l Plan=no\"}, {'Churn=False', 'Area Code=415', \"Int'l Plan=no\"}, {'Churn=False', 'VMail Message=0', 'VMail Plan=no'}, {'VMail Message=0', 'VMail Plan=no', \"Int'l Plan=no\"}, {'Churn=False', 'VMail Message=0', \"Int'l Plan=no\"}, {'Churn=False', 'VMail Plan=no', \"Int'l Plan=no\"}], 4: [{'VMail Message=0', 'Area Code=415', 'VMail Plan=no', \"Int'l Plan=no\"}, {'Churn=False', 'VMail Message=0', 'VMail Plan=no', \"Int'l Plan=no\"}]}\n"
     ]
    }
   ],
   "source": [
    "# minSup=1000\n",
    "data2, s2 = readData2(\"data/churn.txt\")\n",
    "a2 = TP(data=data2, s=s2, minSup=1000)\n",
    "print(a2.miningResults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Lab01 - Frequent itemset mining.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
